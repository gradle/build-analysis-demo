= Gradle Build Analysis Project

The purpose of this project is to provide data for developer productivity by collecting and reporting:

 * What are the costliest tasks (time x frequency) of local developers, by project?
 ** What is the breakdown of these tasks in terms of cache effectiveness, network, something else?
 ** Given a task, tell me the cache rate, mean/stddev/histogram, inputs, network, etc. How has this changed over time?
 * What are the costliest errors (again, time x frequency) and build failures, segmented by environment and failure type?
 ** What is the impact of flaky test failures on local builds?
 * What versions of Gradle BT are in use at Gradle, and with what frequency? Similarly, what versions of guava are in use at Gradle across among active projects?

These applications collect and index Gradle Enterprise and GitHub events data into Google Cloud Storage and BigQuery.

This is separate from the CI Health project because it deals more directly with local development infrastructure.
A project separate from the dotcom/export-api-app was needed to allow this to be presentable for users and to provide real-world feedback on the GE export API.

== Querying the data
You can query build data using:

 * Google Cloud Project: `build-analysis`
 * BigQuery Dataset: `gradle_builds`
 * BigQuery Tables: `builds_YYYYMMDD`

Some fields are JSON. See link:https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions[BigQuery JSON functions] for reference.

== Project structure

=== Build Event Collector app
This application is responsible for streaming build event data from a configured Gradle Enterprise export API endpoint into a specified Google Cloud Storage bucket.
It pulls all builds down and all configured event types, avoiding as much data interaction (parsing, filtering) as possible.

Reference Materials:

 * link:https://docs.gradle.com/enterprise/export-api/[Gradle Enterprise Export API Manual]
 * link:https://cloud.google.com/storage/docs/uploading-objects#storage-upload-object-java[Google Cloud Storage docs]

==== Running Build Event Collector
====sh
gcloud compute instances create build-event-collectorator1 \
   --preemptible \
   --image-family debian-9 \
   --image-project debian-cloud \
   --machine-type n1-highmem-8 \
   --scopes "userinfo-email,cloud-platform" \
   --metadata startup-script='#!/bin/sh
APP_NAME="build-event-collectorator"
APP_VERSION="0.4.1"
export GRADLE_ENTERPRISE_HOSTNAME="gradle-enterprise.mycompany.com"
export GRADLE_ENTERPRISE_USERNAME="my-username"
export GRADLE_ENTERPRISE_PASSWORD="my-password"
export GCS_RAW_BUCKET_NAME="build-events-raw"
gsutil cp "gs://gradle-build-analysis-apps/maven2/org/gradle/buildeng/analysis/${APP_NAME}/${APP_VERSION}/${APP_NAME}-${APP_VERSION}.zip" .
apt-get update && apt-get -y --force-yes install openjdk-8-jdk unzip
update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java
echo "Running ${APP_NAME}-${APP_VERSION}..."
unzip "${APP_NAME}-${APP_VERSION}.zip"
sh "${APP_NAME}-${APP_VERSION}/bin/${APP_NAME}"
echo "Application exited"'
====

By default the collector processes only builds from the moment the app is started, but you can collect past builds by setting `export BACKFILL_DAYS=<number>` in the startup script.

Similarly, you can specify a `export LAST_BUILD_ID="jh4qknspatp2y"` to start streaming from the build _immediately after_ the given build ID.

==== Getting logs for a given instance
====sh
gcloud compute instances get-serial-port-output build-event-collectorator1
====

=== Build Producerator app
This app is not necessary right now. It streamed build events to Google Cloud PubSub to allow fanout with many build event collectors.

Currently the limiting factor is network outbound from the Gradle Enterprise server and multiple downloaders do not make processing faster.

=== Build Event Indexer apps
These applications are responsible for transforming raw data and indexing in Google BigQuery.
Each application consists of a `EventModel` that represents the schema of the BigQuery table to be generated, an `EventJsonTransformer` which filters and transforms data, and a `BuildIndexer` which writes to BigQuery.

You can develop your own indexer by creating a Model, EventsJsonTransformer, and an Indexer.
You will find several indexers for inspiration under `build-event-indexerator/src/main/kotlin`.

[NOTE]
====
We are using FileIO here to read whole files and filter lines rather than reading using TextIO because using TextIO encounters an Error:
       "Total size of the BoundedSource objects generated by split() operation is larger than the allowable limit."
       See link:https://cloud.google.com/dataflow/docs/guides/troubleshooting-your-pipeline#total_number_of_boundedsource_objects_generated_by_splitintobundles_operation_is_larger_than_the_allowable_limit_or_total_size_of_the_boundedsource_objects_generated_by_splitintobundles_operation_is_larger_than_the_allowable_limit[more information].
====

You can run any indexer locally using a task rule and the `DirectRunner`:

[listing]
====
./gradlew :build-event-indexerator:indexBuildEvents --args="--runner=DirectRunner --project=build-analysis --input=gs://build-events-raw/2019/01/01/22*.txt --output=build-analysis:gradle_builds.builds"
====

Once you're happy with your Apache Beam setup, create a Google Dataflow job to run over a larger input.

[listing]
====
./gradlew :build-event-indexerator:indexTestEvents --args="--runner=DataflowRunner --project=build-analysis --input=gs://build-events-raw/2019/01/** --output=build-analysis:gradle_builds.test_executions --region=us-central1 --tempLocation=gs://gradle-dataflow-tmp/$(openssl rand -hex 8)"
====

Let's break this down a bit:

 - `--runner=DataflowRunner` tells Apache Beam that you want to use link:https://console.cloud.google.com/dataflow?project=build-analysis[Google Dataflow] which uses Google Compute Engine under the hood.
 - `--project=build-analysis` configures the Google Cloud project name. We use `build-analysis`.
 - `--input=gs://build-events-raw/2019/01/**` will consume all files from the given link:https://console.cloud.google.com/storage/browser?project=build-analysis[Google Cloud Storage] bucket in the month of January 2019. These build files are keyed by build start time.
 - `--output=build-analysis:gradle_builds.my_builds_table` is the BigQuery table that will be created (if necessary) or appended to.
 - `--region=us-central1` Google Compute region to use for workers. Quotas are set by region, and we have requested increased capacity so that Dataflow can process TBs of build data before Eric dies of old age. This is optional and us-central1 is the default.
 - `--tempLocation=gs://gradle-dataflow-tmp/$(openssl rand -hex 8)` an existing GCS bucket and a random key that Dataflow jobs can use to store temporary files.

// TODO: Monitoring
// schedule daily collector/indexer jobs. See https://cloud.google.com/scheduler/docs/scheduling-instances-with-cloud-scheduler

// TODO: Refactoring
// convert collector to Ratpack and enable compression
// Make type-safe JSON build event model instead of silly guessing

// TODO: Collecting
// Collect data in May-August 2018
// Collect data from December 2018

// TODO: improve indexes
// Index Git commit data from user tags into builds/tests/error tables
// maybe re-index tests data with local changes?
// Index plugin applications data
// reindex Network activities after collecting historical data

// TODO: dashboarding
// Check out Cloud Datalab for viz: https://cloud.google.com/datalab/
// Data Studio dashboard which hits BigQuery

// TODO: documentation
// Blog about all this stuff https://github.com/gradle/blog/issues/136
// "Whatâ€™s the flakiness rate over all branches?"
// "How many flaky tests are there per day/per week over all branches?"
// "Did any tasks become slower over the course of the last weeks?"
// "What is the average download speed from the remote cache? Are there some machines/times when it is slower?"
// "how parallel does work happen inside a Gradle build?"

// IDEA: GZoltar is looking into relating code changes to failures: http://www.gzoltar.com/publications.html
// IDEA: look into BigQueryML for flaky test detection: https://cloud.google.com/bigquery/docs/bigqueryml-scientist-start and https://cloud.google.com/blog/products/gcp/preparing-and-curating-your-data-for-machine-learning
// IDEA: Can we follow a given PR/commit through the CI pipeline? Can we calculate the cost of a given commit or PR?
// IDEA: can we find problematic areas of the codebase by looking at the build data?
// IDEA: calculate the cost of changing a dependency
// IDEA: can we find unnecessary dependencies? Those that are not actually used by the project.

==== Problems we want to catch quickly

* Task has gotten X% and at least Y seconds slower

// TODO: document how to create a partitioned reporting table using bigquery CLI: https://cloud.google.com/bigquery/docs/bq-command-line-tool

[listing]
====
bq --location=[LOCATION] query --destination_table build-analysis:gradle_builds.[TABLE] --time_partitioning_field [COLUMN] --use_legacy_sql=false '[QUERY]'
====

* Build cache effectiveness goes down by X%

==== Activity we want to monitor

* Number of builds (local and CI)

[source,sql]
====
SELECT
  FORMAT_TIMESTAMP('%Y-%m-%d', buildTimestamp) AS day,
  STARTS_WITH(buildAgentId, 'tcagent') AS isCI,
  COUNT(buildId) AS count
FROM
  `gradle_builds.builds`
WHERE
  rootProjectName = 'gradle'
  AND buildTimestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
GROUP BY
  1,
  2
ORDER BY
  1,
  2;
====

* Number of build failures

[source,sql]
====
SELECT
  FORMAT_TIMESTAMP('%Y-%m-%d', buildTimestamp) AS day,
  STARTS_WITH(buildAgentId, 'tcagent') AS isCI,
  COUNT(buildId) AS count
FROM
  `gradle_builds.builds`
WHERE
  rootProjectName = 'gradle'
  AND buildTimestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
  AND BYTE_LENGTH(failureId) > 0
GROUP BY
  1,
  2
ORDER BY
  1,
  2;
====

* What is the impact of flaky test failures on local builds?

// NOTE: TeamCity search "t:flaky" has gives answer for CI
// In order to do this for local builds...

 1) Use GitHub API to get all link:https://github.com/gradle/gradle-private/issues?q=is%3Aopen+sort%3Aupdated-desc+label%3Atype%3Aflaky[flaky test issues]
 2a) Upload issues to BigQuery and make some gnarly SQL that replicates InvalidFailureErrorAnalyzer
 3a) Foolishly assume that test failure data has error messages
 2b) Steal link:https://github.com/gradle/ci-health/blob/master/tagging/src/main/groovy/org/gradle/ci/tagging/flaky/InvalidFailureErrorAnalyzer.groovy[InvalidFailureErrorAnalyzer]
 3b) Side-load known flaky test issues as a Dataflow input and call InvalidFailureAnalyzer from TestIndexer and BuildIndexer

* Where code is changing recently

// TODO: Clever git log formatting
// TODO: Use GitHub API to get latest commit/PR activity

* Can we sessionize builds/commits/events in order to understand workflow?

==== Common ad-hoc queries we want to make

* What versions of library X are in use by frequency?

[source,sql]
====
SELECT
  DISTINCT(CONCAT(dependency.group, ':', dependency.module, ':', dependency.version)),
  b.rootProjectName AS project
FROM
  `gradle_builds.dependencies` AS d,
  UNNEST(moduleDependencies) AS dependency
INNER JOIN
  `gradle_builds.builds` AS b
USING
  (buildId)
WHERE
  dependency.group = 'com.google.guava'
  AND dependency.module = 'guava'
  AND d.buildTimestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY);
====

* How frequent does buildSrc compilation happen locally?

[source,sql]
====
SELECT
  tasks.buildPath,
  tasks.outcome,
  count(buildId) as local_count
FROM
  `gradle_builds.tasks_2019116`,
  UNNEST(tasks) AS tasks
WHERE
  rootProjectName = 'gradle'
  AND buildAgentId NOT LIKE 'tcagent%'
  and tasks.buildPath like ':buildSrc'
GROUP BY
  1, 2
ORDER BY
  3 DESC;
====

* What versions of Gradle are in use recently?

[source,sql]
====
SELECT
  buildToolVersion,
  COUNT(buildId) as count
FROM
  `gradle_builds.builds`
WHERE
  rootProjectName = 'gradle'
  and buildTimestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
GROUP BY
  1
ORDER BY
  2 DESC;
====

* Is any local build still using Java 7? Using Windows? How much memory/CPUs?

[source,sql]
====
SELECT
  JSON_EXTRACT(env.value,
    '$.version') as jdk_version,
  COUNT(env.value) as count
FROM
  `gradle_builds.builds`,
  UNNEST(environmentParameters) AS env
WHERE
  buildAgentId NOT LIKE 'tcagent%'
  AND rootProjectName = 'gradle'
  AND env.key LIKE 'Jvm'
  AND buildTimestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
GROUP BY
  1
ORDER BY
  2 DESC;
====

* Which Gradle features are everyone using? Is everyone using the Daemon?

[source,sql]
====
SELECT
  buildAgentId,
  JSON_EXTRACT(env.value,
    '$.daemon') AS daemon,
  JSON_EXTRACT(env.value,
    '$.taskOutputCache') AS build_cache,
  COUNT(env.value) AS count
FROM
  `gradle_builds.builds`,
  UNNEST(environmentParameters) AS env
WHERE
  buildAgentId NOT LIKE 'tcagent%'
  AND env.key LIKE 'BuildModes'
  and (JSON_EXTRACT(env.value,
    '$.daemon') = 'false' OR JSON_EXTRACT(env.value,
    '$.taskOutputCache') = 'false')
  AND buildTimestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
GROUP BY
  1,
  2,
  3
ORDER BY
  4 DESC;
====

* Most common build cache failures

[source,sql]
====
SELECT
  bc.id,
  ex.message,
  COUNT(bc.id)
FROM
  `gradle_builds.build_cache_interactions` bc
INNER JOIN
  `gradle_builds.exceptions` ex
ON
  bc.failureId = ex.exceptionId
WHERE
  startTimestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
  AND BYTE_LENGTH(failureId) > 0
GROUP BY
  1,
  2
ORDER BY
  3 DESC;
====

* Given a task, tell me the cache rate, mean/stddev/histogram, etc. How has this changed over time?
* Given a test, tell me the outcome history, duration, flakiness, etc.
* What are the costliest tests? Are there Test tasks that never fail? Could we run them less frequently?
* What are the costliest errors (again, time x frequency) and build failures, segmented by environment and failure type?

==== Micro build analysis data applications

* Given an error, have we seen it before?
* How does this build differ from the norm: performance? network? switches?

== Development

=== Prerequisites

 * Gradle Enterprise Export API access
 * Google Cloud build-analysis project access
 * JDK 8 installed

=== Google Cloud initial setup
====sh
gcloud config set compute/region us-central1
====

=== Publishing to Google Cloud

_NOTE: Make sure you're using JDK8 and logged into Google Cloud first._

====sh
./gradlew publish
====

This will publish distZips for all apps to a Maven repository at `gcs://gradle-build-analysis-apps/maven2`
